{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check cuda availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"demo videos/L01.5 Simple Properties of Probabilities - MIT OpenCourseWare.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set video path\n",
    "video_path = \"demo videos/L01.5 Simple Properties of Probabilities - MIT OpenCourseWare.mp4\"\n",
    "\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocess video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_WIDTH = 640\n",
    "DISPLAY_HEIGHT = 480\n",
    "\n",
    "OUTPUT_WIDTH = 1920\n",
    "OUTPUT_HEIGHT = 480\n",
    "OUTPUT_FPS = 30\n",
    "\n",
    "MOTION_THRESHOLD = 5\n",
    "PIXELS_COUNT_THRESHOLD = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FarneBack Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlwit\\.conda\\envs\\vidqaproj\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\mlwit\\.conda\\envs\\vidqaproj\\lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion detected, with flow magnitude: 26.312284469604492\n",
      "Motion detected, with flow magnitude: 27.659997940063477\n",
      "Motion detected, with flow magnitude: 28.271286010742188\n",
      "Motion detected, with flow magnitude: 24.164230346679688\n",
      "Motion detected, with flow magnitude: 22.104902267456055\n",
      "Motion detected, with flow magnitude: 17.64073371887207\n",
      "Motion detected, with flow magnitude: 21.623214721679688\n",
      "Motion detected, with flow magnitude: 19.745525360107422\n",
      "Motion detected, with flow magnitude: 23.459720611572266\n",
      "Motion detected, with flow magnitude: 22.313190460205078\n",
      "Motion detected, with flow magnitude: 23.93301773071289\n",
      "Motion detected, with flow magnitude: 28.29143714904785\n",
      "Motion detected, with flow magnitude: 29.950002670288086\n",
      "Motion detected, with flow magnitude: 32.1051139831543\n",
      "Motion detected, with flow magnitude: 32.241817474365234\n",
      "Motion detected, with flow magnitude: 30.46117401123047\n",
      "Motion detected, with flow magnitude: 32.4863166809082\n",
      "Motion detected, with flow magnitude: 31.013076782226562\n",
      "Motion detected, with flow magnitude: 31.5731143951416\n",
      "Motion detected, with flow magnitude: 26.031021118164062\n",
      "Motion detected, with flow magnitude: 25.677276611328125\n"
     ]
    }
   ],
   "source": [
    "# load video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "# read first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    cap.release()\n",
    "    print(\"Error reading first frame\")\n",
    "    exit()\n",
    "\n",
    "#Resize frame\n",
    "frame1 = cv2.resize(frame1, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "\n",
    "# coonvert first frame to grayscale mode\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# save output\n",
    "if save_output:\n",
    "    out = cv2.VideoWriter('output_farneback_opt_flow.mp4', cv2.VideoWriter_fourcc(*'mp4v'), OUTPUT_FPS, (OUTPUT_WIDTH, OUTPUT_HEIGHT)) # use \"mp4v\" for .mp4 files and \"XVID\" for .avi files\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    # read next frame\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # resize frame\n",
    "    frame2 = cv2.resize(frame2, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "\n",
    "    # convert frame to grayscale\n",
    "    frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # compute optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, frame2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # calculate magnitude and angle of optical flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # create motion binary mask\n",
    "    motion_mask = mag > MOTION_THRESHOLD\n",
    "\n",
    "    # calculate number of pixels with motion\n",
    "    motion_pixels = np.sum(motion_mask)\n",
    "\n",
    "    # detect motion when number of motion pixels exceeds certain threshold\n",
    "    if motion_pixels > PIXELS_COUNT_THRESHOLD:\n",
    "        print(f\"Motion detected, with flow magnitude: {np.mean(mag[motion_mask])}\")\n",
    "\n",
    "    # create a frame copy to draw optical flow vector field\n",
    "    vector_field = frame2.copy()\n",
    "\n",
    "    # set paparams for drawing optical flow vector field\n",
    "    step = 16\n",
    "    h, w = frame2_gray.shape\n",
    "\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            # get the flow vector at (x,y)\n",
    "            fx, fy = flow[y, x]\n",
    "\n",
    "            # draw the arrowed line representing the flow vector\n",
    "            end_point = (int(x + fx), int(y + fy))\n",
    "            cv2.arrowedLine(vector_field, (x, y), end_point, (0, 255, 0), 1, tipLength=0.5)\n",
    "\n",
    "    # convert the binary image mask to bgr mode\n",
    "    motion_mask_bgr = cv2.cvtColor(motion_mask.astype(np.uint8) * 255, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # stack images horizontally for display\n",
    "    display_frame = np.hstack((frame2, vector_field, motion_mask_bgr))\n",
    "\n",
    "    # add text for average motion magnitude\n",
    "    cv2.rectangle(display_frame, (0, 0), (DISPLAY_WIDTH, DISPLAY_HEIGHT//10), (0, 0, 0), -1)\n",
    "    cv2.putText(display_frame, f\"Avg. Motion Magnitude: {np.mean(mag[motion_mask]):.2f}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "    # display frame\n",
    "    cv2.imshow('FarneBack Optical Flow', display_frame)\n",
    "\n",
    "    # upadate previous frame for next iteration\n",
    "    prvs = frame2_gray.copy()\n",
    "\n",
    "    # save output\n",
    "    if save_output:\n",
    "        out.write(display_frame)\n",
    "\n",
    "    # exit if 'ESC' key is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# relsearce resources\n",
    "cap.release()\n",
    "if save_output:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce frame count using motion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9da4605c1f44a64b8324218331103b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/19908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to read frame\n"
     ]
    }
   ],
   "source": [
    "# create frame list\n",
    "frames_list = []\n",
    "\n",
    "# load video to process\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# get the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"failed to read the video.\")\n",
    "    cap.relsease()\n",
    "    exit()\n",
    "\n",
    "# resize_frame\n",
    "frame1 = cv2.resize(frame1, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "\n",
    "# add the first frame to the list\n",
    "frames_list.append((\"00:00\", frame1))\n",
    "\n",
    "# convert the first frame to grayscale\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# get video frame count and initialize progress bar\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "pbar = tqdm(total=frame_count, desc=\"Processing frames\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    pbar.update(1)\n",
    "\n",
    "    # read next frame\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"failed to read frame\")\n",
    "        break\n",
    "\n",
    "    # get current frame timestamp\n",
    "    frame_time_sec = np.round(cap.get(cv2.CAP_PROP_POS_MSEC) / 1000, 1).astype(int)\n",
    "    minutes, seconds = divmod(frame_time_sec, 60) # same as using frame_time_sec//60 and frame_time_sec%60\n",
    "    time_formatted = f\"{int(minutes)}:{int(seconds):02d}\"\n",
    "\n",
    "    # resize frame\n",
    "    frame2 = cv2.resize(frame2, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "\n",
    "    # convert frame to grayscale\n",
    "    frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # compute optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, frame2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # calculate magnitude and angle of optical flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # create motion binary mask\n",
    "    motion_mask = mag > MOTION_THRESHOLD\n",
    "\n",
    "    # calculate number of pixels with motion\n",
    "    motion_pixels = np.sum(motion_mask)\n",
    "\n",
    "    # detect motion when number of motion pixels exceeds certain threshold\n",
    "    if motion_pixels > PIXELS_COUNT_THRESHOLD:\n",
    "        frame2_rgb = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "        frames_list.append((time_formatted, frame2_rgb))\n",
    "\n",
    "    # update previous frame for next iteration\n",
    "    prvs = frame2_gray.copy()\n",
    "\n",
    "# relsearce resources\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video frames count: 19908\n",
      "Reduced video frames count: 271\n"
     ]
    }
   ],
   "source": [
    "# check frame reduction results\n",
    "\n",
    "# load video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# get the length of the video in frames count\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Original video frames count: {frame_count}\\nReduced video frames count: {len(frames_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(frames, fps=30):\n",
    "\n",
    "    # set delay between frames\n",
    "    delay = int(1000/fps)\n",
    "\n",
    "    for frame in frames:\n",
    "        # convert frame to BGR mode\n",
    "        frame_bgr = cv2.cvtColor(frame[1], cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow(\"Video\", frame_bgr)\n",
    "\n",
    "        # press \"ESC\" to exit\n",
    "        if cv2.waitKey(delay) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play reduced video\n",
    "play_video(frames_list, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce video to 1 FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate timestamp difference between frames\n",
    "def calculate_timestamp_diff(time1, time2):\n",
    "\n",
    "    # define timestamp format\n",
    "    timestamp_format = \"%M:%S\"\n",
    "\n",
    "    # convert timestamps to datetime objects\n",
    "    time1 = datetime.strptime(time1, timestamp_format)\n",
    "    time2 = datetime.strptime(time2, timestamp_format)\n",
    "\n",
    "    # calculate timestamp difference\n",
    "    timestamp_diff = abs((time2 - time1).total_seconds())\n",
    "\n",
    "    return timestamp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce video to 1 fps\n",
    "\n",
    "# initialize new frame list\n",
    "frames_list_1_fps = [frames_list[0]]\n",
    "\n",
    "for frame in frames_list[1:]:\n",
    "    timestamp_diff = calculate_timestamp_diff(frame[0], frames_list_1_fps[-1][0])\n",
    "\n",
    "    if timestamp_diff >= 1:\n",
    "        frames_list_1_fps.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous reduced video frames count: 271\n",
      "New reduced video frames count: 85\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous reduced video frames count: {len(frames_list)}\\nNew reduced video frames count: {len(frames_list_1_fps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play reduced video\n",
    "play_video(frames_list_1_fps, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Video Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mono-internVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mean and standard deviation values for ImageNet normalization\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform(input_size):\n",
    "    \"\"\"\n",
    "    Build a transform that resizes the image to the given size and normalizes it\n",
    "    to the range [-1, 1] using the ImageNet mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The size to resize the image to.\n",
    "\n",
    "    Returns:\n",
    "        A callable that takes in an image and applies the specified transform.\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),  # Ensure image is in RGB mode\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),  # Resize to target size\n",
    "        T.ToTensor(),  # Convert image to tensor\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)  # Normalize using ImageNet mean and std\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"\n",
    "    Find the closest aspect ratio to the given aspect ratio in the list of target_ratios.\n",
    "\n",
    "    Args:\n",
    "        aspect_ratio (float): The aspect ratio to find the closest match to.\n",
    "        target_ratios (list): A list of tuples containing the target aspect ratios.\n",
    "        width (int): The width of the original image.\n",
    "        height (int): The height of the original image.\n",
    "        image_size (int): The size of the image to resize to.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the aspect ratio that is closest to the given aspect ratio.\n",
    "    \"\"\"\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    # Iterate through each ratio, finding the closest match\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"\n",
    "    Dynamically preprocess an image by splitting it into tiles of size (image_size, image_size)\n",
    "    and optionally adding a thumbnail of size (image_size, image_size).\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): The image to preprocess.\n",
    "        min_num (int, optional): The minimum number of tiles to split the image into. Defaults to 1.\n",
    "        max_num (int, optional): The maximum number of tiles to split the image into. Defaults to 12.\n",
    "        image_size (int, optional): The size of the tiles to split the image into. Defaults to 448.\n",
    "        use_thumbnail (bool, optional): Whether to add a thumbnail of size (image_size, image_size) to the end of the list of\n",
    "            tiles. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        A list of PIL.Image objects, where each object is a tile of size (image_size, image_size) or a thumbnail of size\n",
    "            (image_size, image_size) if use_thumbnail is True.\n",
    "    \"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image, input_size=448, max_num=12):\n",
    "    \"\"\"\n",
    "    Loads and processes an image from a file.\n",
    "\n",
    "    Args:\n",
    "        image: Image as ndarray.\n",
    "        input_size (int, optional): The size to which the image will be resized. Defaults to 448.\n",
    "        max_num (int, optional): The maximum number of image tiles to process. Defaults to 12.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the processed pixel values of the image.\n",
    "    \"\"\"\n",
    "    image = Image.fromarray(image)\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlwit\\.conda\\envs\\vidqaproj\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e2d94dafe944ae8541f89907a81c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlwit\\.conda\\envs\\vidqaproj\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "path = 'OpenGVLab/Mono-InternVL-2B'\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # You can also use load_in_4bit=True\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(max_new_tokens=256, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dc0f18af1040fe969501e72ace168f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlwit\\.conda\\envs\\vidqaproj\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "# generate caption for each frame\n",
    "captions_list = []\n",
    "for frame in tqdm(frames_list_1_fps[::3]):\n",
    "    pixel_values = load_image(frame[1], max_num=12).to(torch.bfloat16).cuda()\n",
    "\n",
    "    prompt = \"<image>\\nDescribe the image and what is contains, concisely. Start with 'This frame depicts'\"\n",
    "    with torch.no_grad():\n",
    "        response = model.chat(tokenizer, pixel_values, prompt, generation_config)\n",
    "\n",
    "    captions_list.append((frame[0], response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('00:00',\n",
       "  'This frame depicts a sequence of events related to expressions of the atomic axioms, likely from a physics or symbolic algebra course. The content of the image contains mathematical expressions and their corresponding values describing how certain events relate to the properties of atoms.\\n\\nHere is a summary of the equations and their interpretations:\\n\\n1. **P(A) ≥ 0**: This states the probability or probability mass associated with an event happening at atom A.\\n**Interpretation:** This indicates there is a 100% chance that atom A will have a positive value.\\n\\n2. **P(Ω) = 1**: This denotes the probability that state Ω (possibly involving some state) exists and is true.\\n**Interpretation:** This means atom Ω is guaranteed to exist and the condition associated with atom Ω is true.\\n\\n3. **For disjoint events:** Then the formulas for how disjoint events intersect are used in the equation.\\n**P( ΝU B ) = P( Ν ) + P( B )**: **Interpretation:** Here, disjoint events (in this case \"ΝU B\" presumably stands for \"n events that take U into account and B into into consideration\"'),\n",
       " ('0:37',\n",
       "  'This frame depicts a slide from a presentation about some simple consequences of the axioms in mathematics. The slide lists a set of axioms and addresses them.\\n\\n### Axioms\\n- **First axiom:** \\\\( \\\\text{P(A)} \\\\geq 0 \\\\)\\n  - This denotes a rule or proposition stating that if a situation A is true or positive, then the statement P(A) is also true.\\n\\n- **Second axiom:** \\\\( \\\\text{P(A)} \\\\leq 1 \\\\)\\n  - This denotes a rule or proposition stating that if a situation A is not true but instead small compared to 1 (i.e., is neither significantly small nor truly impossible), then the statement P(A) is also true.\\n\\n### Consequences\\n1. **P(A) = 1**:\\n   - This statement represents a consequent result of the initial axiom. If P(A) is equal to 1, it means that the initial rule P(A) is simultaneously true or negative. This can be related to situations where no action (such as no effort or no effort but remaining the case of P(A) ≤ 1) results in a value of P(A) equal to 1.'),\n",
       " ('1:05',\n",
       "  'This frame depicts a slide with an educational or explanatory content, focusing on the concepts of axioms and consequences. It includes a mathematical definition and examples related to sets and disjoint events.'),\n",
       " ('1:31',\n",
       "  'This frame depicts a slide with a title \"Some simple consequences of the axioms\" and two sections, \"Axioms\" and \"Consequences.\"'),\n",
       " ('2:10',\n",
       "  'This frame depicts an educational slide explaining the simple consequences of different types of axioms. The slide details various axioms (P* - Point A > 0, P* - Point A ≤ 1, P({\\\\phi}) = 0) and their respective consequences (P(A < U B) = P(A) + P(AU B) = P(A) + P(AU BU C) = P(A) + P(A + B)C, and similar for k- Disjoint events). The slide concludes with an expression to define the simple consequences of \\\\( P(s1, s2,..., sk) = P({s1}) + P({s2}) +... + P({sk}) \\\\).'),\n",
       " ('2:46',\n",
       "  'This frame depicts a text slide, which appears to contain a mathematical definition and its implications. The slide is titled \"Some simple consequences of the axioms.\"\\n\\nHere is a more detailed description:\\n\\n### Content Description\\nThe slide presents an axiom, denoted simply as \\\\( A \\\\):\\n\\n- \\\\(\\\\text{P}(\\\\mathbf{A}) \\\\geq 0 \\\\) \\n    - Implies: \\\\(\\\\ \\\\text{P}(\\\\mathbf{U} B) = \\\\text{P}(\\\\mathbf{A}) + \\\\text{P}(\\\\mathbf{B}) \\\\)\\n\\n### Breakdown of Symbols and Logic:\\n- **P(\\\\mathbf{A})**: A primary axiom. However, the expression next to it involves two additional axioms:\\n  - \\\\(\\\\text{P}(\\\\mathbf{U} B) = \\\\text{P}(\\\\mathbf{A}) + \\\\text{P}(\\\\mathbf{B}) \\\\)\\n- **P(\\\\mathbf{A}) \\\\geq 0**: Represents a statement that \\\\(\\\\mathbf{A}\\\\) is non-negative. Axiom, \\\\(\\\\text{P}(\\\\mathbf{U} B'),\n",
       " ('2:54',\n",
       "  'This frame depicts a slide from a presentation, explaining a concept related to simple consequences of axioms, including axioms and disjoint events, axioms (represented by A), and the principle \"P(A \\\\underline{B}) = P(A) + P(B).\"'),\n",
       " ('2:58',\n",
       "  'This frame depicts a whiteboard with the heading \"Some simple consequences of the axions\" at the top. Next to it, there are three key points with mathematical expressions and corresponding color-coded conditions:\\n\\n1. Axioms: \\n   - A) \\\\(P(A) \\\\geq 0\\\\)\\n   - B) \\\\(P(\\\\hat{S}) = 1\\\\)\\n2. Disjoint Events:\\n   - \\\\(P(A \\\\cup B) = P(\\\\hat{A}) + P(\\\\hat{B})\\\\)\\n\\nThe whiteboard includes a sketch of a two-dimensional matrix or graph, along with text to describe the axes: \\n- Axiom A: A geometric structure depicted with the point marked `P\\' inside a square.\\n- Axiom B: Another point marked \\'A\\' on the same grid, with a label above the figure.\\n- Axiom C: A third point marked \\'B\\' on the grid.\\n\\nThe image depicts the logical relationship between these axioms and demonstrates the concept of disjoint events.'),\n",
       " ('3:10',\n",
       "  'This frame shows a whiteboard that contains the following text and mathematical expressions related to the properties of a certain geometric shape, which appears to be a rhombus with axes labeled A, B, and C.\\n\\nKey Points on the Whiteboard:\\n\\n1. A rhombus with a given diagonal \\\\(A B\\\\) and given diagonal \\\\(BC = AB-BA\\\\).\\n2. Diagonals \\\\(A C\\\\) and \\\\(B C\\\\) of the rhombus are given as \\\\(AC=1\\\\) and \\\\(BC=1\\\\).\\n3. A right triangle \\\\(ABP\\\\) is drawn along the diagonal \\\\(AB\\\\).\\n4. The equation \\\\( P(A) = 0 \\\\), meaning \\\\(P(A) = 0\\\\).\\n\\nAdditionally, the slide has a section labeled \"For disjoint events,\" introducing a geometric property related to the areas of two intersecting diagonals in the rhombus.\\n\\nThe mathematical expression for the area of one diagonal and the other diagonal of the rhombus is presented:\\n\\n\\\\[ Area = \\\\frac{1}{2} \\\\times DiagonalA_1 \\\\times DiagonalA_2 \\\\]\\n\\\\[ \\\\text{Diagonal } A_1 = A \\\\cdot B \\\\]\\nSince the area ('),\n",
       " ('3:20',\n",
       "  'This frame depicts a hand-drawn schematic of two simple geometric shapes (a square with a right angle and an equilateral triangle), along with accompanying text and symbols related to the properties of these shapes.\\n\\nNote: The hand-drawn diagram (c) contains a simplified representation without numerical annotations and assumes the values “2” and “0”. The values indicated in the provided text are actual real mathematical expressions for the properties of the geometric shapes mentioned in the diagram.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Q&A / Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key = groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# respond to query based on captions\n",
    "\n",
    "def generate_response(captions, query, groq_client):\n",
    "    captions_list_to_text = \"/n\".join([\"[\" + cap[0] + \"] \" + cap[1] for cap in captions])  # [00:00] caption 0\\n[00:01] caption 1 ...\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Based on the following chronologically sorted frame captions, where each line has a timestamp and a frame caption, {query}: {captions_list_to_text}\",\n",
    "            }\n",
    "        ],\n",
    "        model = \"llama-3.1-70b-versatile\"\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize this educational video into chapters and maintain the chronological order\"\n",
    "answer = generate_response(captions_list, query, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video appears to be an educational presentation on simple consequences of axioms, focusing on various mathematical concepts. Here's a summary of the video in chronological order, divided into chapters:\n",
      "\n",
      "**Chapter 1: Introduction to Axioms (00:00 - 00:37)**\n",
      "\n",
      "The video starts by introducing axioms related to atomic expressions, probability, and disjoint events. It explains the basic concepts of axioms and how they are used to describe atomic behavior.\n",
      "\n",
      "**Chapter 2: Axioms and Consequences (00:37 - 2:10)**\n",
      "\n",
      "This chapter delves deeper into the axioms and their consequences. It discusses the first and second axioms, which state that the probability of an event is greater than or equal to 0 and less than or equal to 1, respectively.\n",
      "\n",
      "**Chapter 3: Disjoint Events and Axioms (2:10 - 2:58)**\n",
      "\n",
      "This chapter explains disjoint events and how they relate to axioms. It discusses the concept of disjoint events and how they can be represented using mathematical equations.\n",
      "\n",
      "**Chapter 4: Geometric Consequences of Axioms (2:58 - 3:54)**\n",
      "\n",
      "This chapter explores the geometric consequences of axioms, using concepts like points, lines, and planes to illustrate the ideas.\n",
      "\n",
      "**Chapter 5: Simple Consequences of Axioms (3:54 - 5:59)**\n",
      "\n",
      "This chapter discusses simple consequences of axioms, including concepts like Pascal's Triangle and the properties of disjoint events.\n",
      "\n",
      "**Chapter 6: Angle Sum Identities and Axioms (5:59 - 7:20)**\n",
      "\n",
      "This chapter explains angle sum identities and how they relate to axioms. It discusses the concept of angle sum identities and how they can be used to derive simple consequences of axioms.\n",
      "\n",
      "**Chapter 7: Advanced Consequences of Axioms (7:20 - 9:28)**\n",
      "\n",
      "This chapter delves into more advanced consequences of axioms, including concepts like vector addition and scalar multiplication.\n",
      "\n",
      "**Chapter 8: Conclusion (9:28 - 11:04)**\n",
      "\n",
      "This chapter concludes the video by summarizing the main concepts discussed. It reiterates the importance of understanding axioms and their consequences in various mathematical contexts.\n",
      "\n",
      "**Chapter 9: Unrelated Content (11:04 - end)**\n",
      "\n",
      "The final chapter appears to be unrelated to the rest of the video, featuring a human body in a full-length view.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: \n",
      "\n",
      "**Question 1.** Suppose you have three disjoint events A, B, and C. The probability of A, P(A), is 0.4, the probability of B, P(B), is 0.3, and the probability of C, P(C), is 0.5. What is the probability of the union of A, B, and C, P(A U B U C)?\n",
      "\n",
      "**A)** 0.2\n",
      "**B)** 0.4\n",
      "**C)** 1.2\n",
      "**D)** 0.7\n"
     ]
    }
   ],
   "source": [
    "query = \"Generate a test question\"\n",
    "answer = generate_response(captions_list, query, client)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three test questions with answers based on the provided frame captions:\n",
      "\n",
      "**Test Question 1**\n",
      "What is the interpretation of the equation P(A) ≥ 0 in the context of atomic axioms?\n",
      "\n",
      "A) There is a 0% chance that atom A will have a positive value.\n",
      "B) There is a 100% chance that atom A will have a positive value.\n",
      "C) There is a 50% chance that atom A will have a positive value.\n",
      "D) There is a negative chance that atom A will have a positive value.\n",
      "\n",
      "**Answer:** B) There is a 100% chance that atom A will have a positive value.\n",
      "\n",
      "**Test Question 2**\n",
      "What is the interpretation of the equation P(Ω) = 1 in the context of atomic axioms?\n",
      "\n",
      "A) There is a 0% chance that atom Ω will exist and the condition associated with atom Ω is true.\n",
      "B) There is a 100% chance that atom Ω will exist and the condition associated with atom Ω is true.\n",
      "C) There is a 50% chance that atom Ω will exist and the condition associated with atom Ω is true.\n",
      "D) There is a negative chance that atom Ω will exist and the condition associated with atom Ω is true.\n",
      "\n",
      "**Answer:** B) There is a 100% chance that atom Ω will exist and the condition associated with atom Ω is true.\n",
      "\n",
      "**Test Question 3**\n",
      "What is the formula for the probability of disjoint events in the context of atomic axioms?\n",
      "\n",
      "A) P(A ∪ B) = P(A) - P(B)\n",
      "B) P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\n",
      "C) P(A ∪ B) = P(A) + P(B)\n",
      "D) P(A ∪ B) = P(A) * P(B)\n",
      "\n",
      "**Answer:** C) P(A ∪ B) = P(A) + P(B)\n"
     ]
    }
   ],
   "source": [
    "query = \"Generate three test questions with answers\"\n",
    "answer = generate_response(captions_list, query, client)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidqaproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
